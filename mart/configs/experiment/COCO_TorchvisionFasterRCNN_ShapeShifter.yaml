# @package _global_

defaults:
  - /attack/perturber@model.modules.perturber: default
  - /attack/perturber/initializer@model.modules.perturber.initializer: uniform
  - /attack/perturber/composer@model.modules.perturber.composer: color_jitter_warp_overlay
  - /attack/perturber/projector@model.modules.perturber.projector: range
  - /attack/optimizer@model.optimizer: sgd
  - /attack/gradient_modifier@model.gradient_modifier: lp_normalizer
  - override /datamodule: coco
  - override /model: torchvision_faster_rcnn
  - override /metric: average_precision
  - override /optimization: super_convergence
  - override /callbacks: [model_checkpoint, lr_monitor, perturbation_visualizer]

task_name: "COCO_TorchvisionFasterRCNN_ShapeShifter"
tags: ["adv"]

optimized_metric: "test_metrics/map"

callbacks:
  model_checkpoint:
    monitor: "validation_metrics/map"
    mode: "min"

trainer:
  # 117,266 training images, 6 epochs, batch_size=2, 351798
  max_steps: 351798
  # FIXME: "nms_kernel" not implemented for 'BFloat16', torch.ops.torchvision.nms().
  precision: 32
  track_grad_norm: 2

datamodule:
  num_workers: 8
  ims_per_batch: 2

model:
  modules:
    perturber:
      size: [3, 416, 416]

      initializer:
        min: 127
        max: 129

      composer:
        warp:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.RandomErasing
              p: 0.75
              scale: [0.2, 0.7]
              ratio: [0.3, 3.3]
            - _target_: torchvision.transforms.RandomAffine
              degrees: [-5, 5]
              scale: [0.3, 0.5]
              shear: [-3, 3, -3, 3]
              interpolation: 2 # BILINEAR
        clamp: [0, 255]
        brightness: [0.5, 1.5]
        contrast: [0.5, 1.5]
        saturation: [0.5, 1.5]
        hue: [-0.05, 0.05]
        pixel_scale: 255

    losses_and_detections:
      model:
        model:
          weights: "FasterRCNN_ResNet50_FPN_Weights.COCO_V1"

  # FIXME: Would be much nicer if we could specify _freeze_: True for each module
  freeze: "losses_and_detections"

  optimizer:
    lr: 25.5
    momentum: 0.9

  gradient_modifier:
    p: inf

  training_sequence:
    seq005: perturber

    seq010:
      preprocessor: ["perturber"]

  validation_sequence: ${.training_sequence}

  test_sequence: ${.validation_sequence}
