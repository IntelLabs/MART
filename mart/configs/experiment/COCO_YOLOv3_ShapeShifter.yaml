# @package _global_

defaults:
  - /attack/perturber@model.modules.perturber: default
  - /attack/perturber/initializer@model.modules.perturber.initializer: uniform
  - /attack/perturber/composer@model.modules.perturber.composer: color_jitter_warp_composite
  - /attack/perturber/projector@model.modules.perturber.projector: range
  - /attack/gradient_modifier@model.gradient_modifier: lp_normalizer
  - override /optimization: super_convergence
  - override /datamodule: coco_yolov3
  - override /model: yolov3
  - override /metric: average_precision
  - override /callbacks: [perturbation_visualizer, lr_monitor, override_mode]

task_name: "COCO_YOLOv3_ShapeShifter"
tags: ["adv"]

optimized_metric: "test_metrics/map"

trainer:
  # 64115 training images, batch_size=16, FLOOR(64115/16) = 4007
  max_steps: 40070 # 10 epochs
  # mAP can be slow to compute so limit number of images
  limit_val_batches: 100
  limit_test_batches: 100
  precision: 32

callbacks:
  perturbation_visualizer:
    frequency: 500

  override_mode:
    # YOLOv3 uses training/eval modes to switch functionality. We disable this and just always use training mode.
    training_mode: "train"
    validation_mode: "train"
    test_mode: "train"

datamodule:
  num_workers: 32
  ims_per_batch: 16

  train_dataset:
    annFile: ${paths.data_dir}/coco/annotations/person_instances_train2017.json
  val_dataset:
    annFile: ${paths.data_dir}/coco/annotations/person_instances_val2017.json
  test_dataset:
    annFile: ${paths.data_dir}/coco/annotations/person_instances_val2017.json

model:
  modules:
    perturber:
      size: [3, 416, 234]

      initializer:
        min: 0.49
        max: 0.51

      projector:
        min: 0.0
        max: 1.0

      composer:
        warp:
          _target_: torchvision.transforms.Compose
          transforms:
            - _target_: torchvision.transforms.RandomErasing
              p: 0.75
              scale: [0.2, 0.7]
              ratio: [0.3, 3.3]
            - _target_: torchvision.transforms.RandomAffine
              degrees: [-5, 5]
              scale: [0.3, 0.5]
              shear: [-3, 3, -3, 3]
              interpolation: 2 # BILINEAR
        clamp: [0, 1]
        brightness: [0.5, 1.5]
        contrast: [0.5, 1.5]
        saturation: [0.5, 1.5]
        hue: [-0.05, 0.05]
        pixel_scale: 1.0
        use_masks: True

    loss:
      weights: [1, 1, 1e-5]

  freeze: "yolov3"

  load_state_dict:
    yolov3: ${paths.data_dir}/yolov3_original.pt

  optimizer:
    lr: 0.01
    momentum: 0.9

  gradient_modifier: null

  training_sequence:
    seq005: perturber
    seq010:
      yolov3: ["perturber.input_adv"]
    seq030:
      loss:
        - losses.hide_target_objects_loss
        - losses.correct_target_class_loss
        - perturber.total_variation
    seq050:
      output:
        total_variation: perturber.total_variation

  training_step_log:
    - loss
    - total_loss
    - coord_loss
    - obj_loss
    - noobj_loss
    - class_loss
    - hide_objects_loss
    - target_class_loss
    - hide_target_objects_loss
    - correct_target_class_loss
    - target_count
    - score_count
    - target_score_count
    - total_variation

  training_metrics: null

  validation_sequence:
    seq005: perturber
    seq010:
      yolov3: ["perturber.input_adv"]
    seq030:
      loss:
        - losses.hide_target_objects_loss
        - losses.correct_target_class_loss
        - perturber.total_variation

  test_sequence:
    seq005: perturber
    seq010:
      yolov3: ["perturber.input_adv"]
    seq030:
      loss:
        - losses.hide_target_objects_loss
        - losses.correct_target_class_loss
        - perturber.total_variation
