# @package _global_

defaults:
  - /attack/perturber@model.modules.perturbation: default
  - /attack/perturber/initializer@model.modules.perturbation.initializer: uniform
  - /attack/perturber/projector@model.modules.perturbation.projector: range
  - /attack/composer@model.modules.input_adv: warp_composite
  - /attack/gradient_modifier@model.gradient_modifier: lp_normalizer
  - override /datamodule: coco_yolo
  - override /model: yolo
  - override /optimization: super_convergence
  - override /metric: average_precision
  - override /callbacks:
      [
        model_checkpoint,
        lr_monitor,
        perturbation_visualizer,
        gradient_monitor,
        attack_in_eval_mode,
        no_grad_mode,
      ]

task_name: "COCO_YOLOv3_ShapeShifter"
tags: ["adv"]

optimized_metric: "test_metrics/map"

trainer:
  # 64115 training images, batch_size=8, FLOOR(64115/16) = 8014
  max_steps: 80140 # 10 epochs
  # mAP can be slow to compute so limit number of images
  limit_val_batches: 100
  precision: 32

callbacks:
  model_checkpoint:
    monitor: "validation_metrics/map"
    mode: "min"

  attack_in_eval_mode:
    module_classes:
      - _target_: hydra.utils.get_class
        path: torch.nn.BatchNorm2d

  no_grad_mode:
    module_names: "model.yolo"

  perturbation_visualizer:
    perturbation: "model.perturbation.perturbation"
    frequency: 500

datamodule:
  num_workers: 4
  ims_per_batch: 8

  train_dataset:
    annFile: ${paths.data_dir}/coco/annotations/person_instances_train2017.json
  val_dataset:
    annFile: ${paths.data_dir}/coco/annotations/person_instances_val2017.json
  test_dataset:
    annFile: ${paths.data_dir}/coco/annotations/person_instances_val2017.json

model:
  modules:
    empty_targets:
      _target_: mart.nn.EmptyTargets

    yolo:
      config_path: ${paths.data_dir}/yolov3.cfg
      weights_path: ${paths.data_dir}/yolov3.weights

    perturbation:
      size: [3, 416, 234]

      initializer:
        min: 0.49
        max: 0.51

      projector:
        min: 0.0
        max: 1.0

    total_variation:
      _target_: mart.nn.TotalVariation

    input_adv:
      warp:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: mart.transforms.ColorJitter
            brightness: [0.5, 1.5]
            contrast: [0.5, 1.5]
            saturation: [0.5, 1.0]
            hue: [-0.05, 0.05]
          - _target_: torchvision.transforms.RandomAffine
            degrees: [-5, 5]
            translate: [0.1, 0.25]
            scale: [0.4, 0.6]
            shear: [-3, 3, -3, 3]
            interpolation: 2 # BILINEAR
      clamp: [0, 1]

  optimizer:
    lr: 0.05
    momentum: 0.9

  lr_scheduler:
    scheduler:
      three_phase: true

  gradient_modifier: null

  training_sequence:
    seq004:
      empty_targets:
        targets: "target.list_of_targets"
    seq005: "perturbation"
    seq006: "input_adv"
    seq010:
      yolo:
        images: "input_adv"
        targets: "empty_targets"
    seq050:
      total_variation:
        _call_with_args_:
          - "perturbation"
    seq100:
      loss:
        _call_with_args_:
          - "yolo.confidence"
          - "total_variation"
        weights:
          - 1
          - 0.0001

  training_metrics: null
  training_step_log:
    total_variation: "total_variation"

  validation_sequence:
    seq004: ${..training_sequence.seq004}
    seq005: ${..training_sequence.seq005}
    seq006: ${..training_sequence.seq006}
    seq050: ${..training_sequence.seq050}

  test_sequence:
    seq005: ${..training_sequence.seq005}
    seq006: ${..training_sequence.seq006}
