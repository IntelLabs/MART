defaults:
  - modular

# All three sequences of training/validation/test are (almost, except for the training loss) equivalent but in different syntax.

# The verbose version.
training_sequence:
  seq010:
    preprocessor:
      _call_with_args_: ["input"]
  seq020:
    logits:
      _call_with_args_: ["preprocessor"]
  seq030:
    loss:
      _call_with_args_: ["logits", "target"]
  seq040:
    preds:
      _call_with_args_: ["logits"]
  seq050:
    output:
      {
        "preds": "preds",
        "target": "target",
        "logits": "logits",
        "loss": "loss",
      }

# The kwargs-centric version.
# We may use *args as **kwargs to avoid the lengthy _call_with_args_.
#   The drawback is that we would need to lookup the *args names from the code.
# We use a list-style sequence since we don't care about replacing any elements.
validation_sequence:
  - preprocessor:
      tensor: input
  - logits: ["preprocessor"]
  - preds:
      input: logits
  - output:
      preds: preds
      target: target
      logits: logits

# The simplified version.
#   We treat a list as the `_call_with_args_` parameter.
test_sequence:
  seq010:
    preprocessor: ["input"]
  seq020:
    logits: ["preprocessor"]
  seq030:
    preds: ["logits"]
  seq040:
    output: { preds: preds, target: target, logits: logits }

modules:
  preprocessor: ???

  logits: ???

  loss:
    _target_: torch.nn.CrossEntropyLoss

  preds:
    _target_: torch.nn.Softmax
    dim: 1

  output:
    _target_: mart.nn.ReturnKwargs
